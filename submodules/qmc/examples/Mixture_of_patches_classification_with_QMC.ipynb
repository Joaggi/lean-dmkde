{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/fagonzalezo/qmc/blob/master/examples/Mixture_of_patches_classification_with_QMC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ9Ku1h_aYwR"
      },
      "source": [
        "# Mixture of patches classification with QMC\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tc-zDLY4AUB",
        "outputId": "3a28b670-85cd-412f-ccfd-99f821b39006"
      },
      "outputs": [],
      "source": [
        "# Install qmc if running in Google Colab\n",
        "\n",
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install --upgrade  git+https://github.com/fagonzalezo/qmc.git\n",
        "    !pip install -U tensorflow-addons\n",
        "else:\n",
        "    import sys\n",
        "    sys.path.insert(0, \"../\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dwx1YXdUaYwT"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "v0DXdtVWaYwU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "# import tensorflow_addons as tfa\n",
        "import qmc.tf.layers as layers\n",
        "import qmc.tf.models as models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6B8j9dBaYwU"
      },
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zt6i10fJaYwV",
        "outputId": "3d05b08b-0f80-4a55-bc7d-501f77ebe015"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1) - y_train shape: (60000,)\n",
            "x_test shape: (10000, 28, 28, 1) - y_test shape: (10000,)\n"
          ]
        }
      ],
      "source": [
        "num_classes = 10\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "x_train = np.expand_dims(x_train, axis=-1)\n",
        "x_test = np.expand_dims(x_test, axis=-1)\n",
        "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0JLTS8WaYwV"
      },
      "source": [
        "## Configure the hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "B50WUbDRaYwV"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "batch_size = 256\n",
        "num_epochs = 10\n",
        "image_size = 28  # We'll resize input images to this size\n",
        "patch_size = 7  # Size of the patches to be extract from the input images\n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "projection_dim = 64\n",
        "input_dim = 64\n",
        "num_rff = 512\n",
        "gamma = 2**-5\n",
        "n_comp = 80\n",
        "random_state = 0\n",
        "\n",
        "mlp_head_units = [256]  # Size of the dense layers of the final classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD5GJabEaYwV"
      },
      "source": [
        "## Use data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ESXjO5kiaYwW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-11-17 22:13:57.533050: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "2021-11-17 22:13:57.535500: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
          ]
        }
      ],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        keras.layers.Normalization(),\n",
        "        #layers.Resizing(image_size, image_size),\n",
        "        #layers.RandomFlip(\"horizontal\"),\n",
        "        #layers.RandomRotation(factor=0.02),\n",
        "        #layers.RandomZoom(\n",
        "        #    height_factor=0.2, width_factor=0.2\n",
        "        #),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "# Compute the mean and the variance of the training data for normalization.\n",
        "data_augmentation.layers[0].adapt(x_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Me6HwOyFaYwW"
      },
      "source": [
        "## Implement multilayer perceptron (MLP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "z6mukUVmaYwW"
      },
      "outputs": [],
      "source": [
        "\n",
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKNUm55DaYwW"
      },
      "source": [
        "## Implement patch creation as a layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KGUSdfNNaYwX"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Patches(keras.layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, num_patches, patch_dims])\n",
        "        return patches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqA3m5C6aYwX"
      },
      "source": [
        "Let's display patches for a sample image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "iytQ3v-EaYwX",
        "outputId": "4b1127c2-5815-4475-fa14-bcdb4c7c3dad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image size: 28 X 28\n",
            "Patch size: 7 X 7\n",
            "Patches per image: 16\n",
            "Elements per patch: 49\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAFBklEQVR4nO3dy6vmcxzA8efMGCeXSGmmFDGcsVMujUY2FkpKlBSRBRZukbJSSlkaVkLzB9hIkURJIgmRFLJwadxqFhJh3M48Flajeb4n5+K8z5zXa3k+/X7nV6d33zqfnt8zN51OJ0DPlvV+AODIxAlR4oQocUKUOCHqmNHw8i3X+VcurLFXDj0zd6SfOzkhSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHHrPcDsHF8/eAlw/kndzwxnO+5//bh/KSn3/7Pz3Q0c3JClDghSpwQJU6IEidEiROixAlR9pwcZuuO7TNn99zw/PDaVw9uHc5PefWL4XxxON18nJwQJU6IEidEiROixAlR4oQoqxQO8/XN58yc3XryS8Nrd79343C+/cCny3qmzcrJCVHihChxQpQ4IUqcECVOiBInRNlzcpjTrvhq2df+8skpq/gkODkhSpwQJU6IEidEiROixAlR4oQoe85N5uDVu4fzl899fObswOIfw2sXnvpmOP9rOOXfnJwQJU6IEidEiROixAlR4oQocUKUPecms+ehd4fzbXOzv8bvqg9uG167fb/30q4mJydEiROixAlR4oQocUKUOCFKnBBlz3mUmdt27HB+/Jafl33vXz/yXtr/k5MTosQJUeKEKHFClDghSpwQZZVylPn84QuH8xdOnf3qy8lkMnngwEUzZ+fs8+rL/5OTE6LECVHihChxQpQ4IUqcECVOiLLn3GDm5ueH8+euf2yJO4w/Uvbis3tmzk7f/9YS92Y1OTkhSpwQJU6IEidEiROixAlR4oQoe84N5qdrzh/Od21b2S7yjL3vz5xNV3Rn/isnJ0SJE6LECVHihChxQpQ4IUqcEGXPucGce9/HK7v+uTuH84U/3l3R/Vk9Tk6IEidEiROixAlR4oQocUKUOCHKnjPm4NW7h/N9pz+5ovuf9ewS36I59anNCicnRIkTosQJUeKEKHFClDghyiol5oeFlf1J9v145nA+/+GXw/niin47q8nJCVHihChxQpQ4IUqcECVOiBInRNlzroOtu86eOXvj3r1LXD0/nD762pXD+cL37yxxfyqcnBAlTogSJ0SJE6LECVHihChxQpQ95zr47JbtM2cnbhnvMX8+9PtwvnCXPebRwskJUeKEKHFClDghSpwQJU6IEidE2XOug8X55X/N3sVv3jGcnzX5cNn3psXJCVHihChxQpQ4IUqcECVOiLJKWQOLl10wnL9+7ej1l8cNr5374vhlPBEbkZMTosQJUeKEKHFClDghSpwQJU6IsudcAz+dMX695Y6ts3eZd3976fDanQ9/MJwfGk7ZSJycECVOiBInRIkTosQJUeKEKHFClD3nGjjhpu+G89+nf86cffTIecNrT/zNV/xtFk5OiBInRIkTosQJUeKEKHFClDghyp5zHZz/xu0zZzufscfkH05OiBInRIkTosQJUeKEKHFClDghyp5zDRx7+f7hfOdkPIfJxMkJWeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiJqbTqfr/QzAETg5IUqcECVOiBInRIkTosQJUX8DrkZ4LohBfcoAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 288x288 with 16 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(4, 4))\n",
        "image = x_train[np.random.choice(range(x_train.shape[0]))]\n",
        "plt.imshow(image.reshape([image_size, image_size]))\n",
        "plt.axis(\"off\")\n",
        "\n",
        "resized_image = tf.image.resize(\n",
        "    tf.convert_to_tensor([image]), size=(image_size, image_size)\n",
        ")\n",
        "patches = Patches(patch_size)(resized_image)\n",
        "print(f\"Image size: {image_size} X {image_size}\")\n",
        "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
        "print(f\"Patches per image: {patches.shape[1]}\")\n",
        "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
        "\n",
        "n = int(np.sqrt(patches.shape[1]))\n",
        "plt.figure(figsize=(4, 4))\n",
        "for i, patch in enumerate(patches[0]):\n",
        "    ax = plt.subplot(n, n, i + 1)\n",
        "    patch_img = tf.reshape(patch, (patch_size, patch_size, 1))\n",
        "    plt.imshow(patch_img.numpy().astype(\"uint8\").reshape([patch_size, patch_size]))\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3vrJRFsaYwX"
      },
      "source": [
        "## Implement the patch encoding layer\n",
        "\n",
        "The `PatchEncoder` layer will linearly transform a patch by projecting it into a\n",
        "vector of size `projection_dim`. In addition, it adds a learnable position\n",
        "embedding to the projected vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SlVB9myqaYwY"
      },
      "outputs": [],
      "source": [
        "positional_embeddings = np.zeros((num_patches, projection_dim))\n",
        "for position in range(num_patches):\n",
        "    for i in range(0, projection_dim, 2):\n",
        "       positional_embeddings[position, i] = (\n",
        "                                          np.sin(position / (10000 ** ((2*i) / projection_dim)))\n",
        "                                            )\n",
        "       positional_embeddings[position, i + 1] = (\n",
        "                                              np.cos(position / (10000 ** ((2 * (i + 1) ) / projection_dim)))\n",
        "                                                )\n",
        "\n",
        "\n",
        "class PatchEncoder(keras.layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = tf.Variable(initial_value=positional_embeddings,\n",
        "                                              dtype=tf.float32, \n",
        "                                              trainable=False)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "        self.position_embedding = tf.Variable(initial_value=positional_embeddings,\n",
        "                                      dtype=tf.float32, \n",
        "                                      trainable=False)\n",
        "\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        #encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        #encoded = self.projection(patch) + self.position_embedding\n",
        "        encoded = self.projection(patch)\n",
        "        return encoded\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ct37nEzaYwY"
      },
      "source": [
        "## Build QMC model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Jp57jQYuaYwY"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_QMC_model():\n",
        "    inputs = keras.layers.Input(shape=input_shape)\n",
        "    # Augment data.\n",
        "    augmented = data_augmentation(inputs)\n",
        "    # Create patches.\n",
        "    patches = Patches(patch_size)(augmented)\n",
        "    # Encode patches.\n",
        "    # encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "    fm_x1 = layers.QFeatureMapRFF(49, dim=num_rff , gamma=gamma)\n",
        "    psi_x = tf.transpose(fm_x1(patches), perm=[0, 2, 1])\n",
        "    ones = tf.ones_like(psi_x[:, 0:1, :], dtype=tf.float32) / num_patches\n",
        "    rho_x = tf.keras.layers.concatenate((ones, psi_x), axis=1)\n",
        "    qmdmc = layers.QMClassifSDecompFDMatrix(dim_x=num_rff, dim_y=10, n_comp=n_comp)\n",
        "    rho_y = qmdmc(rho_x)\n",
        "    y_w = rho_y[:, 0, :] # shape (b, d_in)\n",
        "    y_v = rho_y[:, 1:, :] # shape (b, dim_x, d_in)\n",
        "    probs = tf.einsum('...j,...ij,...ij->...i', y_w, y_v, tf.math.conj(y_v))\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=probs)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 766
        },
        "id": "r8NtRtZWL0ym",
        "outputId": "b29f382d-d677-4d8b-e107-472f69bd50f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_13\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_17 (InputLayer)           [(None, 28, 28, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "data_augmentation (Sequential)  (None, 28, 28, 1)    3           input_17[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "patches_17 (Patches)            (None, 16, 49)       0           data_augmentation[16][0]         \n",
            "__________________________________________________________________________________________________\n",
            "q_feature_map_rff_16 (QFeatureM (None, 16, 512)      25600       patches_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.transpose_10 (TFOp (None, 512, 16)      0           q_feature_map_rff_16[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_37 (Sl (None, 1, 16)        0           tf.compat.v1.transpose_10[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "tf.ones_like_15 (TFOpLambda)    (None, 1, 16)        0           tf.__operators__.getitem_37[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf.math.truediv_14 (TFOpLambda) (None, 1, 16)        0           tf.ones_like_15[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_13 (Concatenate)    (None, 513, 16)      0           tf.math.truediv_14[0][0]         \n",
            "                                                                 tf.compat.v1.transpose_10[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "qm_classif_s_decomp_fd_matrix_1 (None, 11, 80)       41840       concatenate_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_39 (Sl (None, 10, 80)       0           qm_classif_s_decomp_fd_matrix_11[\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_38 (Sl (None, 80)           0           qm_classif_s_decomp_fd_matrix_11[\n",
            "__________________________________________________________________________________________________\n",
            "tf.math.conj_10 (TFOpLambda)    (None, 10, 80)       0           tf.__operators__.getitem_39[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf.einsum_10 (TFOpLambda)       (None, 10)           0           tf.__operators__.getitem_38[0][0]\n",
            "                                                                 tf.__operators__.getitem_39[0][0]\n",
            "                                                                 tf.math.conj_10[0][0]            \n",
            "==================================================================================================\n",
            "Total params: 67,443\n",
            "Trainable params: 67,440\n",
            "Non-trainable params: 3\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = create_QMC_model()\n",
        "model.summary()\n",
        "model.layers[3].trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raJsjdl3aYwY"
      },
      "source": [
        "## Compile, train, and evaluate the mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrA5SfEhaYwY",
        "outputId": "9179efdb-27b8-42a9-d139-d469bd8efd1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "211/211 [==============================] - 17s 79ms/step - loss: 1.8974 - accuracy: 0.4679 - val_loss: 1.6858 - val_accuracy: 0.6455\n",
            "Epoch 2/10\n",
            "211/211 [==============================] - 17s 80ms/step - loss: 1.6734 - accuracy: 0.6248 - val_loss: 1.6238 - val_accuracy: 0.6765\n",
            "Epoch 3/10\n",
            "211/211 [==============================] - 17s 80ms/step - loss: 1.6241 - accuracy: 0.6591 - val_loss: 1.5908 - val_accuracy: 0.6373\n",
            "Epoch 4/10\n",
            "211/211 [==============================] - 17s 81ms/step - loss: 1.5975 - accuracy: 0.6801 - val_loss: 1.5638 - val_accuracy: 0.7112\n",
            "Epoch 5/10\n",
            "211/211 [==============================] - 17s 80ms/step - loss: 1.5787 - accuracy: 0.6982 - val_loss: 1.5483 - val_accuracy: 0.7022\n",
            "Epoch 6/10\n",
            "211/211 [==============================] - 17s 80ms/step - loss: 1.5661 - accuracy: 0.7074 - val_loss: 1.5376 - val_accuracy: 0.7578\n",
            "Epoch 7/10\n",
            "211/211 [==============================] - 18s 84ms/step - loss: 1.5534 - accuracy: 0.7141 - val_loss: 1.5271 - val_accuracy: 0.7108\n",
            "Epoch 8/10\n",
            "211/211 [==============================] - 17s 81ms/step - loss: 1.5440 - accuracy: 0.7242 - val_loss: 1.5212 - val_accuracy: 0.7173\n",
            "Epoch 9/10\n",
            "211/211 [==============================] - 17s 83ms/step - loss: 1.5363 - accuracy: 0.7227 - val_loss: 1.5159 - val_accuracy: 0.7372\n",
            "Epoch 10/10\n",
            "211/211 [==============================] - 17s 80ms/step - loss: 1.5289 - accuracy: 0.7303 - val_loss: 1.5120 - val_accuracy: 0.7348\n",
            "313/313 [==============================] - 1s 5ms/step - loss: 1.5575 - accuracy: 0.7348\n",
            "Test accuracy: 73.48%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def run_experiment(model):\n",
        "    optimizer = tf.optimizers.Adam(\n",
        "        learning_rate=learning_rate\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
        "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[checkpoint_callback],\n",
        "    )\n",
        "\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "    _, accuracy = model.evaluate(x_test, y_test)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "QMC_model = create_QMC_model()\n",
        "history = run_experiment(QMC_model)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Mixture of patches classification with QMC",
      "provenance": []
    },
    "environment": {
      "name": "tf2-gpu.2-4.m61",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m61"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
